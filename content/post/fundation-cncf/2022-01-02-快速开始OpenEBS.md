---
layout:     post 
slug:      "quick-start-openebs"
title:      "快速开始OpenEBS"
subtitle:   ""
description: ""
date:       2022-01-02
author:     "梁远鹏"
image: "[img/post-bg-2015.jpg](https://res.cloudinary.com/lyp/image/upload/v1581729516/hugo/blog.github.io/avian-beak-bird-blur-416117.jpg)"
published: true
tags:
    - openebs 
    - cncf
categories: [ CloudNative ]
---

# 前言  


# 前提  

- 使用OpenEBS需要机器已经安装了`iSCSI` 
- 已经安装了kubernetes

ubuntu可以用下面的命令安装和开启:  

```shell
sudo apt-get update
sudo apt-get install open-iscsi
sudo systemctl enable --now iscsid
```
  
centos系列系统可以用下面的命令安装和开启:  
```shell
yum install iscsi-initiator-utils -y
sudo systemctl enable --now iscsid
```  

# 安装OpenEBS

## Helm方式  

```
helm repo add openebs https://openebs.github.io/charts
helm repo update
helm install openebs --namespace openebs openebs/openebs --create-namespace
```  

上述命令将会安装OpenEBS并且默认安装Jiva和Local PV组件,如果需要其他的存储引擎可以用下面的安装命令开启:   

```
helm install openebs --namespace openebs openebs/openebs --create-namespace --set cstor.enabled=true
```  

等待一会后会出现helm安装成功的提示:   

```shell
oem@lan:~/k8s/openebs$ helm install openebs --namespace openebs openebs/openebs --create-namespace --set cstor.enabled=true

NAME: openebs
LAST DEPLOYED: Mon Jan  3 00:28:41 2022
NAMESPACE: openebs
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
Successfully installed OpenEBS.

Check the status by running: kubectl get pods -n openebs

The default values will install NDM and enable OpenEBS hostpath and device
storage engines along with their default StorageClasses. Use `kubectl get sc`
to see the list of installed OpenEBS StorageClasses.

**Note**: If you are upgrading from the older helm chart that was using cStor
and Jiva (non-csi) volumes, you will have to run the following command to include
the older provisioners:

helm upgrade openebs openebs/openebs \
        --namespace openebs \
        --set legacy.enabled=true \
        --reuse-values

For other engines, you will need to perform a few more additional steps to
enable the engine, configure the engines (e.g. creating pools) and create 
StorageClasses. 

For example, cStor can be enabled using commands like:

helm upgrade openebs openebs/openebs \
        --namespace openebs \
        --set cstor.enabled=true \
        --reuse-values

For more information, 
- view the online documentation at https://openebs.io/ or
- connect with an active community on Kubernetes slack #openebs channel.
```

可以看一下openebs相关的日志运行情况,耐心等待容器运行成功就可以了,第一次需要拉去镜像,所以需要一点时间:   

运行命令`watch kubectl get po -n openebs`  

```
Every 2.0s: kubectl get po -n openebs                                                                                       lan: Mon Jan  3 00:30:41 2022

NAME                                            READY   STATUS              RESTARTS   AGE
openebs-cstor-admission-server-f7c94c6d-n82n6   0/1     ContainerCreating   0          118s
openebs-cstor-csi-controller-0                  0/6     ContainerCreating   0          118s
openebs-cstor-csi-node-hk6kg                    0/2     ContainerCreating   0          118s
openebs-cstor-cspc-operator-d576bb58d-cqcln     0/1     ContainerCreating   0          118s
openebs-cstor-cvc-operator-867b5697b5-glh76     1/1     Running             0          118s
openebs-localpv-provisioner-6f686f7697-967kj    0/1     ContainerCreating   0          118s
openebs-ndm-operator-5948569558-frhjd           0/1     ContainerCreating   0          118s
openebs-ndm-pjfmz                               1/1     Running             0          118s
```  

过了一会可能会出现失败,原因是CSI的镜像的官方地址是`k8s.gcr.io`,国内访问不了,不过没关系,这时候只需要将`k8s.gcr.io`修改为`lank8s.cn`就可以了.  

```shell 
oem@lan:~/repo/git$ helm upgrade openebs --namespace openebs openebs/openebs --create-namespace --set cstor.enabled=true --set csiController.resizer.registry=lank8s.cn --set csiNode.driverRegistrar.image=lank8s.cn 
coalesce.go:200: warning: cannot overwrite table with non table for image (map[pullPolicy:IfNotPresent registry:k8s.gcr.io/ repository:sig-storage/csi-node-driver-registrar tag:v2.3.0])
```   

设置后发现无法生效,目前已经在`slack`给`openebs`提问是否没有开放这个设置.




同时可以运行`kubectl get storageClass`看看storageClass:  


```
NAME                 PROVISIONER          RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
longhorn (default)   driver.longhorn.io   Delete          Immediate              true                   4d3h
openebs-device       openebs.io/local     Delete          WaitForFirstConsumer   false                  3m12s
openebs-hostpath     openebs.io/local     Delete          WaitForFirstConsumer   false                  3m12s
```  

# 试用动态PVC  

当openebs的所有pod都处于Running的时候就已经真正安装OpenEBS成功了,接下来试用一下动态PVC的效果.  

以`redis`作为有状态服务部署.  

redis.yaml:   

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app: redis
  name: redis
spec:
  selector:
    matchLabels: 
      app: redis
  serviceName: redis-svc
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
        - image: redis:6.2.4
          name: redis
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: redis-vct
              mountPath: /data
  volumeClaimTemplates:
    - metadata:
        name: redis-vct
      spec:
        storageClassName: openebs-jiva-csi-default
        accessModes: ["ReadWriteOnce"]
        resources:
          requests:
            storage: 1Gi
---
apiVersion: v1
kind: Service
metadata:
  name: redis
  labels:
    app: redis
spec:
  ports:
    - port: 6379
      name: redis
  type: ClusterIP
  selector:
    app: redis
```  

写好yaml文件后运行`kubectl apply -f redis.yaml`部署redis.  

redis的pod运行起来后进到redis存一条数据:  

1. kubectl exec -it redis-0 sh  
2. redis-cli 
3. set lank8scn 1  

这时候数据已经存在pvc里面了,可以验证一下,将`redis-0`这个pod删除,如果`重建`后的`redis-0`这个pod还有`lank8scn`这个key的数据,那么说明数据已经持久化成功了.  

1. kubectl delete po redis-0  
2. kubectl exec -it redis-0 sh(等待redis的pod成功运行后)
3. redis-cli
4. get lank8scn  

顺利的话你得到key的数据:   
```shell
127.0.0.1:6379> get lank8scn
"1"
```  

这时候说明已经使用openebs作为storageclass并且动态生成PVC成功了.

# 再简化  

可以思考一下,在上述一整个过程当中哪一个步骤是最麻烦的?  

在我看来是当部署helm chart时镜像中包含了`k8s.gcr.io`或`gcr.io`等国内不可访问的镜像后去helm chart模板里面翻到底是哪个参数可以 将这个镜像仓库地址修改的时候.  

本次部署openebs的时候我就经过了一番折腾才最终找到这个修改的地方.  

首先我去openebs的slack提问,如果有人回答我那就太好了(很遗憾,到最后也没有人给我回答),同时去找到openebs对应的helm chart仓库,一步一步翻看这个镜像是哪里用到了,如何通过参数修改.  

对于只有很长时间内只有这么一次经历其实也还好,但是我们难免还是会在其他常用的helm chart中遇到这样的问题,在这里我推荐一下[在线的lank8s webhook服务](https://liangyuanpeng.com/post/cncf-kubernetes/deploy-lank8s-webhook-for-k8s.gcr.io/),只需要在k8s集群当中部署一个`MutatingWebhook`就可以了,不再需要到处找参数去修改.  

当然也正在持续的做些准备,将这个webhook的所有内容都开源出来并且手把手教学如何在不可访问外网的k8s集群中使用这个webhook,可以持续关注上面文章的地址,当做好准备时将会更新上述文章.  

如果使用了这个webhook后(不论在线服务还是本地部署),你不再需要担心哪里会用到不可访问的`k8s.gcr.io`或`gcr.io`的镜像,轻松解决全局问题.  

以本文为例子的话那就是直接运行`helm install openebs --namespace openebs  --create-namespace --set cstor.enabled=true openebs/openebs`就可以了,不需要那些特殊设置镜像的参数了.

# 记录 

完整命令设置命令  

```
helm install openebs --namespace openebs openebs/openebs --create-namespace --set csiController.resizer.registry=lank8s.cn  --set csiController.snapshotter.registry=lank8s.cn  --set csiController.snapshotController.registry=lank8s.cn  --set csiController.attacher.registry=lank8s.cn    --set csiController.provisioner.registry=lank8s.cn  --set csiNode.driverRegistrar.image=lank8s.cn
```

# 注意

本文还在持续编写中
